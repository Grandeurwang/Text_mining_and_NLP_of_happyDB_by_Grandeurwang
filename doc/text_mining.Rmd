---
title: "Text mining of happyDB"
author: Guanren Wang
output: html_notebook
---

HappyDB is a crowd-sourced happy moments. The goal of the corpus is to advance the state of the art of understanding the causes of happiness that can be gleaned from text.

The researchers collected the happy moments from over 10,000 people living in 101 countries (most are from USA and India) and collect over a corpus of 100,000+ happy moments.Plus, the data in the demographic dataset contains many useful statistics like gender, age, marital and parenthood from which we can get more insights of joy.

To get a more accurate result, I lemmatized all the tokens generated by clean_hm.

```{r warning=FALSE, message=FALSE,echo=FALSE,fig.align="center"}
#step-1 load preprocessed data and packages

cleaned<-read.csv("D:/applied data science/1st project--NLP+exploratory data analysis/clean-hm+keywords.csv",stringsAsFactors = FALSE)
cleaned<-cleaned[,-4]
vad<-read.csv('D:/applied data science/happyDB/happydb/vad.csv',stringsAsFactors = FALSE)
demog<-read.csv('D:/applied data science/happyDB/happydb/demographic.csv')

demog$gender<-as.character(demog$gender)
demog$gender[demog$gender=='']<-'unknown'
demog$gender[demog$gender=='f']<-'female'
demog$gender[demog$gender=='m']<-'male'
demog$gender[demog$gender=='o']<-'other'
demog$gender<-as.factor(demog$gender)

library(tm)
library(tidytext)
library(tidyverse)
library(DT)
```

```{r warning=FALSE, message=FALSE,echo=FALSE}
#step-2 lemmatize the tokens
library(textstem)
cleaned<-cleaned%>%
  mutate(text=lemmatize_strings(text))
```
Here's a small part of data:
```{r warning=FALSE, message=FALSE,echo=FALSE}
library(DT)
datatable(cleaned%>%
            inner_join(demog)%>%
            head(n=5))
```
#Metrics of happiness---Sentiment Analysis

When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust.
One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words.

there are a variety of methods and dictionaries that exist for evaluating the opinion or emotion in text. Here I used three lexicons in the sentiments dataset:

·AFINN from Finn Årup Nielsen\
·bing from Bing Liu and collaborators\
·nrc from Saif Mohammad and Peter Turney\


###Average happiness scores of different genders
I calculated the happiness score for every happy moment and then calculated the average scores for each gender. From the graph we could find that there is **no apparent difference of happiness between male and female over lifetime**. But the happiness of other gender varies over time. It is noticed that certain average scores of unknown gender are negative. The reason maybe that some people didn't  write this conscientiously.
```{r warning=FALSE, message=FALSE,echo=FALSE,fig.align="center"}
#step-3 tokenization and join table with dictionary built in tidytext
tokenized<-cleaned%>%unnest_tokens(word,text)

sdict<-bind_rows(get_sentiments('bing'),get_sentiments('nrc')%>%
                   filter(sentiment%in%c('positive','negative')))%>%
          distinct()

nrc_bing<-tokenized%>%
  inner_join(sdict)

sent_analy_nb<-nrc_bing%>%
  count(hmid,wid,sentiment,sort=T)%>%
  spread(sentiment,n,fill=0)%>%
  mutate(score=positive-negative)%>%
  inner_join(vad)%>%
  inner_join(demog)

afinn<-tokenized%>%
  inner_join(get_sentiments('afinn'))

sent_analy_afinn<-afinn%>%
  group_by(hmid,wid)%>%
  summarise(score=sum(score))%>%
  inner_join(vad)%>%
  inner_join(demog)
```

```{r warning=FALSE, message=FALSE,echo=FALSE,fig.align="center"}
#step-4 explore whether gender has an impact on happiness

#create categories to classify age
sent_analy_nb$age<-as.numeric(sent_analy_nb$age)
age_avgscore<-sent_analy_nb%>%
  mutate(agegroup=cut(age,seq(0,100,10)))%>%
  group_by(agegroup,gender)%>%
  summarise(avgscore=mean(score))%>%
  na.omit()

sent_analy_afinn$age<-as.numeric(sent_analy_afinn$age)
age_avgscore_afinn<-sent_analy_afinn%>%
  mutate(agegroup=cut(age,seq(0,100,10)))%>%
  group_by(agegroup,gender)%>%
  summarise(avgscore=mean(score))%>%
  na.omit()

#visualize the avg across gender based on nrc&bing
library(ggplot2)
ggplot(age_avgscore)+geom_col(aes(x=agegroup,y=avgscore,fill=gender))+
  theme_bw()+
  facet_wrap(~gender,ncol=2)+
  scale_x_discrete(labels=c('10','20','30','40','50','60','70','80','90','100'))+
  labs(title = 'Average happiness score based on nrc&bing dict',subtitle = NULL)+xlab('Age group')+ylab('Average score')

#visualize the avg across gender based on afinn
ggplot(age_avgscore_afinn)+geom_col(aes(x=agegroup,y=avgscore,fill=gender))+
  theme_bw()+
  facet_wrap(~gender,ncol=2)+
  scale_x_discrete(labels=c('10','20','30','40','50','60','70','80','90','100'))+
  labs(title = 'Average happiness score based on afinn dict',subtitle = NULL)+xlab('Age group')+ylab('Average score')
```

###The most important words of happy moments---Term Frequency(TF)
The first plot shows the top 10 words that contribute most to negative sentiment and positive sentiment respectively. Obviously, positive words have higher contribution. However, there are still some negative words appear. The reason is that positive words like enjor and fun are not the only way to express our happinss, sometimes we may use not/no + negative vocabulary to describe our delight.

```{r warning=FALSE, message=FALSE,echo=FALSE,fig.align="center"}
# step-5 explore factors that contibute to happiness in general
wd_cld<-tokenized%>%
  inner_join(sdict)%>%
  anti_join(stop_words)%>%
  count(word,sentiment,sort=T)

#correct the mistakes in the dictionary
wd_cld[wd_cld$word=='mother',2]<-'positive'
wd_cld[wd_cld$word=='funny',2]<-'positive'
wd_cld[wd_cld$word=='boy',2]<-'positive'
wd_cld<-unique(wd_cld)

#plot word count
wd_cld%>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  theme_bw()+
  coord_flip()
```
###Wordcloud
Here is the wordcloud plot of happiness contribution. The bigger size a word is, the more important a word is. Besides, words with different contribution have different colors. We could find that the most important words are **firend,enjoy,birthday,daughter,love,nice,win,job,favorite**,etc.
```{r warning=FALSE, message=FALSE,echo=FALSE,fig.align="center"}
#plot wordcloud
library(wordcloud)
set.seed(126)
pal <- brewer.pal(6,"Dark2")
wd_cld%>%
  anti_join(stop_words)%>%
  with(wordcloud(word, n, max.words = 100,random.color = F,random.order = F,colors = pal))
```
##Explore relationships between words
Sometimes correlation of words is also very important. For example, some words tend to follow others immediately, or tend to co-occur within the same documents. 

I only selected bigrams appeared over 100 times. In this graph, every red dot represent a word. The darker an edge is, the higher frequency a bigram is. Arrows point to the following words. For example, at the top of graph we could see word *fall* points to word *asleep*, so the order should be *fall asleep*. 

We could find something very interesting from this graph: many of words with high frequency (the biggest words in wordlcoud) are in the core area of a network. **Friend** and **birthday** connect 2 words respectively with a dark edge.**Job,favorite** and **dinner** are also in the center of their networks.

What's more, we could conclude some common things make people happy: *job offer, job interview, eat dinner, cook dinner, watch TV, ice cream, ride bike, video game, basketball game* etc.
```{r warning=FALSE, message=FALSE,echo=FALSE,fig.align="center"}
# step-6 build the network of words
# use bigram to tokenize and do lemmatization
bigram_sep<-cleaned%>%
  unnest_tokens(word,cleaned_hm,token = 'ngrams',n=2)%>%
  
  #separate bigram
  
  separate(word,c('word1','word2'),sep=' ')%>%
  mutate(word1=lemmatize_strings(word1),word2=lemmatize_strings(word2))%>%
  count(word1,word2,sort = T)

#remove stop words and count
bigram_sep<-bigram_sep%>%
  filter(!(word1 %in% stop_words$word | word2 %in% stop_words$word))%>%
  filter(n>100)

#visualize the relationships by igraph and ggraph
library(igraph)
library(ggraph)
library(viridisLite)
library(viridis)
g<-bigram_sep%>%
  graph_from_data_frame()
a <- grid::arrow(type = "closed", length = unit(.05, "inches"))

set.seed(124)
ggraph(g, layout = "fr") +
  geom_edge_link(aes(edge_color = log(n)), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "red", size = 2) +
  geom_node_text(aes(label = name,alpha=0.25), vjust = 1,show.legend = FALSE,repel = TRUE)+
  theme_void()+
  labs(title = 'Relationships of words')

```
#Mining latent information behind the happiness---Topic Modelling

We often have collections of documents that we’d like to divide into natural groups so that we can understand them separately. In this dataset, 98% of happy moments are from India and USA. So I utilized **LDA** model,a method for unsupervised classification of such documents, to discover more difference between American's happiness and Indian's happiness.
```{r warning=FALSE, message=FALSE,echo=FALSE}
# step-7 topic modelling using LDA

#tidy the data frame and denote USA as document 1,IND as document 2 using cleaned data
cleaned_demog<-cleaned%>%
  inner_join(demog)%>%
  filter(country=='USA'|country=='IND')%>%
  unnest_tokens(word,cleaned_hm)%>%
  anti_join(stop_words)%>%
  mutate(word=lemmatize_strings(word))%>%
  count(word,country,sort = T)%>%
  mutate(doc=ifelse(country=='USA',1,2))%>%
  select(-country)
  

#convert tidy form data into dtm
library(topicmodels)
dtm<-cleaned_demog%>%
  cast_dtm(document = doc,term = word,value = n)

#implement LDA
#preliminary setting of Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#number of topics
k=5

lda_output<-dtm%>%
  LDA(k=k,method = 'Gibbs',control=list(nstart=nstart,
                                                 seed = seed, best=best,
                                                 burnin = burnin,iter = iter,                                                  thin=thin))

#check gamma
gamma <- tidy(lda_output, matrix = "gamma")%>%
  mutate(country=ifelse(document==1,'USA','IND'))%>%
  select(-document)


#chech beta
beta <- tidy(lda_output, matrix = "beta")%>%
  group_by(topic)%>%
  arrange(topic,desc(beta))%>%
  top_n(10,wt=beta)

topic<-augment(lda_output,data=dtm)%>%
  mutate(country=ifelse(document==1,'USA','IND'))%>%
  mutate(country=as.factor(country),topic=as.factor(.topic))%>%
  select(-document,-.topic)%>%
  mutate(count=ifelse(country=='USA',count/10,count))%>%
  group_by(country,topic)%>%
  top_n(5,wt=count)
```
###Gamma
Gamma represents estimated proportion of words from that country that are generated from that topic.
```{r warning=FALSE, message=FALSE,echo=FALSE}
datatable(gamma)
```
###Beta
Beta represents the probability of that term being generated from that topic
```{r warning=FALSE, message=FALSE,echo=FALSE}
datatable(beta)
```

*Topic 1* (70% of words in happy moments from USA are from *Topic 1*,while only 20% of words in happy moments from India are from *Topic 1*)has: "day"    "dinner" "find"   "friend" "happy"  "month"  "play"   "time" "watch"  "week";
```{r warning=FALSE, message=FALSE,echo=FALSE}
set.seed(126)
pal <- brewer.pal(6,"Dark2")
beta%>%filter(topic==1)%>%
  with(wordcloud(term, beta, max.words = 100,random.color = F,random.order = F,colors = pal))
```

*Topic 2* (25% of words in happy moments from USA are from *Topic 1*,while only 1% of words in happy moments from India are from *Topic 1*) has: "lunch"     "life"         "book"      "sister"    "movie"    "cat"       "happiness"  "trip"      "cook" ;
```{r warning=FALSE, message=FALSE,echo=FALSE}
set.seed(126)
pal <- brewer.pal(6,"Dark2")
beta%>%filter(topic==2&term!='2')%>%
  with(wordcloud(term, beta, max.words = 100,random.color = F,random.order = F,colors = pal))
```


*Topic 3* (74% of words in happy moments from India are from *Topic 3*, while only 0.6% of words in happy moments from India are from *Topic 3*) has : "day"    "enjoy"  "family" "feel"   "friend" "happy"  "life"   "moment" "month"  "time".
```{r warning=FALSE, message=FALSE,echo=FALSE}
set.seed(126)
pal <- brewer.pal(6,"Dark2")
beta%>%filter(topic==3)%>%
  with(wordcloud(term, beta, max.words = 100,random.color = F,random.order = F,colors = pal))
```
Thus, *Topic 1* and *Topic 2* are more American, while *Topic 2* is more Indian, which could be used to differentiate USA and India.

###Conclusion

*Topic 1* has dinner, find, play, watch, week which *Topic 3* doesn't have, meaning **Americans tend to gain happiness by themselves from entertainment like play and watch TV (TV often follows watch according to network graph) from *Topic 1* and trip and movie from *Topic 2***;\
*Topic 3* has enjoy, family, feel, life, moment which *Topic 1* doesn't have, **indicating Indians are more likely to enjoy their life and  spend time with family**.